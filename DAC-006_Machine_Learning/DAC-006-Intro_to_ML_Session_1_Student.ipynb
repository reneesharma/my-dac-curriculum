{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** \\\n",
    "Machine Learning focuses on building systems that can learn from and make decisions based on data. Instead of being explicitly programmed to perform a task, a machine learning model is trained on a dataset to identify patterns and make predictions or decisions without human intervention.\n",
    "\n",
    "**A general pipeline:** \\\n",
    "1.) Data Collection - Collecting data relevant to the problem you want to solve. \\\n",
    "2.) Training  - Using this dataset to train a model, which involves adjusting the model's parameters to minimize errors in its predictions. \\\n",
    "3.) Evaluation - Testing the model on new, unseen data to evaluate its performance. \\\n",
    "4.) Prediction + Deployment - Using the trained model to make predictions or decisions on new data, and deploying it so that other users may use it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate (Simple) Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate linear regression is a statistical method used to model the relationship between a single independent variable $x$ and a dependent variable $y$ by fitting a linear equation to observed data. It aims to predict the dependent variable based on the value of the independent variable.\n",
    "\n",
    "equation of line: $$y_i = wx_i + b + \\epsilon_i$$\n",
    "equation of your prediction: $$ \\hat{y_i} = wx_i + b$$\n",
    "\n",
    "How do you generate a line? You need a value for the Slope and Intercept. Use Least Squares method / Maximum Likelihood Estimation to determine. \n",
    "\n",
    "Objective: \\\n",
    "Minimise the sum of squared error terms ie: $$ \\min_{w,b} \\sum_i \\epsilon_i^2 = \\min_{w,b} \\sum_i (y_i - \\hat{y}_i)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identical to the univariate case except you're now modelling the relationship between $y$ with multiple other independent variables/features $x_1,x_2...x_n$.\n",
    "\n",
    "equation of line: $$y_i = w_1x_{1,i} + w_2x_{2,i} + ...  + b + \\epsilon_i$$\n",
    "equation of your prediction: $$ \\hat{y_i} = w_1x_{1,i} + w_2x_{2,i} + ... + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two metrics can be used **Mean Squared Error (MSE)** and **$R^2$ value**.\n",
    "\n",
    "We split our model into training data and validation/test data. Apply regression line fitted on train data into validation data to evaluate performance. MSE is pretty self explanatory, $R^2$ measures the proportion of variance of the dependent/target feature that is explained by the independent features. \n",
    "\n",
    "$$\n",
    " MSE = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2 \\\\\n",
    " R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n",
    "$$\n",
    "\n",
    "MSE is also known as the loss function - a function that maps events or values of variables onto a real number intuitively representing some \"cost\" associated with the event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting happens when your model fits the training data too well; performs worse on test data. \\\n",
    "Regularization tries to prevent this by adding a penalty term to a model's loss function.\n",
    "\n",
    "In Linear Regression:\n",
    "$$\n",
    "\\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2 + \\text{Regularization Term}\n",
    "$$\n",
    "Your left term reduces losses, right term prevents losses from decreasing excessively. Now let's see this in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = ax + b + Gaussian noise \n",
    "# def reg_data(a, b, n, s):\n",
    "#    rtn_x, rtn_y = [], []\n",
    "#    for i in range(n):\n",
    "#        x = np.random.normal(0.0, 0.5)\n",
    "#        y = a * x + b + np.random.normal(0.0, s)\n",
    "#        rtn_x.append(x) # input features\n",
    "#        rtn_y.append(y) # target values\n",
    "#    return np.array(rtn_x).reshape(-1,1), np.array(rtn_y)\n",
    "\n",
    "# # Generate 1,000 data points drawn from y = ax + b + noise\n",
    "# # s : standard deviation of the noise distribution\n",
    "# x, y = reg_data(a=0.5, b=0.3, n=1000, s=0.2)\n",
    "\n",
    "# # y = w0 + w1*x1 + w2*x2 + ... â†’ w0*x0 + w1*x1 + w2*x2 + ... (x0 = 1)\n",
    "# # y = [w0, w1, w2, ...] * [x0, x1, x2, ...].T  (T : transpose)\n",
    "# # y = W * X.T\n",
    "# X = np.hstack([np.ones([x.shape[0], 1]), x]) # horizontally stack a column of ones (intercept) with input features\n",
    "# REG_CONST = 0.01   # regularization constant\n",
    "\n",
    "# # Loss function : Mean Squared Error\n",
    "# def ols_loss(W, args):\n",
    "#     e = np.dot(W, X.T) - y\n",
    "#     mse = np.mean(np.square(e))  # mean squared error\n",
    "#     loss = mse + REG_CONST * np.sum(np.square(W)) # this is Ridge (L2) Regularization\n",
    "    \n",
    "#     # save W and loss\n",
    "#     if args[0] == True:\n",
    "#         trace_W.append([W, loss])\n",
    "#     return loss\n",
    "\n",
    "# # Perform optimization process\n",
    "# trace_W = []\n",
    "# result = optimize.minimize(ols_loss, [-4., 4], args=[True]) # minimise loss function starting from initial weights [-4,4]\n",
    "# print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for more info on the output visit: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "# # x: is the vector of the optimal solution\n",
    "\n",
    "# # Plot the training data and draw the regression line.\n",
    "# y_hat = np.dot(result.x, X.T) # predicted values using the optimized weights and the design matrix\n",
    "# plt.figure(figsize=(6, 6)) \n",
    "# plt.scatter(x, y, s=5, c='r')\n",
    "# plt.plot(x, y_hat, c='blue')\n",
    "# plt.axvline(x=0, ls='--', lw=0.5, c='black')\n",
    "# plt.axhline(y=0, ls='--', lw=0.5, c='black')\n",
    "# plt.show()\n",
    "\n",
    "# # Draw the loss function and the path to the optimal point.\n",
    "# m = 5\n",
    "# t = 0.1\n",
    "# w0, w1 = np.meshgrid(np.arange(-m, m, t), np.arange(-m, m, t))\n",
    "# zs = np.array([ols_loss([a,b], [False]) for [a, b] in zip(np.ravel(w0), np.ravel(w1))])\n",
    "# z = zs.reshape(w0.shape)\n",
    "\n",
    "# fig = plt.figure(figsize=(7, 7))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Draw the surface of the loss function\n",
    "# ax.plot_surface(w0, w1, z, alpha=0.7)\n",
    "\n",
    "# # Draw the path to the optimal point.\n",
    "# b = np.array([tw0 for [tw0, tw1], td in trace_W])\n",
    "# w = np.array([tw1 for [tw0, tw1], td in trace_W])\n",
    "# d = np.array([td for [tw0, tw1], td in trace_W])\n",
    "# ax.plot(b, w, d, marker='o', color=\"r\")\n",
    "\n",
    "# ax.set_xlabel('W0 (bias)')\n",
    "# ax.set_ylabel('W1 (slope)')\n",
    "# ax.set_zlabel('distance')\n",
    "# ax.azim = -50\n",
    "# ax.elev = 50\n",
    "# plt.show()\n",
    "\n",
    "# # Check the R2 score\n",
    "# sst = np.sum(np.square(y - np.mean(y)))  # total sum of squares\n",
    "# sse = np.sum(np.square(y - y_hat))       # sum of squares of error\n",
    "# r2 = 1 - sse / sst\n",
    "# print('\\nR2 score = {:.4f}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Implementation in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling (Normalization/Standardization) is a technique that shifts data closer toward the origin and scales the different feature $x_i, x_j$ weights to ensure that they are not significantly different. If they are different, can also affect the estimation of our slope $w$ and intercept $b$. \\\n",
    "During regularization, it may also unfairly impose greater penalties on some coefficients over others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore the Boston.csv dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read our data into a dataframe \n",
    "data=pd.read_csv('datasets/Boston.csv')\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are what the column names represent:\n",
    "* CRIM - per capita crime rate by town\n",
    "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS - proportion of non-retail business acres per town.\n",
    "* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "* NOX - nitric oxides concentration (parts per 10 million)\n",
    "* RM - average number of rooms per dwelling\n",
    "* AGE - proportion of owner-occupied units built prior to 1940\n",
    "* DIS - weighted distances to five Boston employment centres\n",
    "* RAD - index of accessibility to radial highways\n",
    "* TAX - full-value property-tax rate per $10,000\n",
    "* PTRATIO - pupil-teacher ratio by town\n",
    "* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* LSTAT - % lower status of the population\n",
    "* MEDV - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     crim    zn  indus  chas    nox     rm   age     dis  rad  \\\n",
       "0           1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1   \n",
       "1           2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2   \n",
       "2           3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2   \n",
       "3           4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3   \n",
       "4           5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3   \n",
       "\n",
       "   tax  ptratio  lstat  medv  \n",
       "0  296     15.3   4.98  24.0  \n",
       "1  242     17.8   9.14  21.6  \n",
       "2  242     17.8   4.03  34.7  \n",
       "3  222     18.7   2.94  33.4  \n",
       "4  222     18.7   5.33  36.2  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of the dataframe (try DataWrangler)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      int64\n",
       "crim          float64\n",
       "zn            float64\n",
       "indus         float64\n",
       "chas            int64\n",
       "nox           float64\n",
       "rm            float64\n",
       "age           float64\n",
       "dis           float64\n",
       "rad             int64\n",
       "tax             int64\n",
       "ptratio       float64\n",
       "lstat         float64\n",
       "medv          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis',\n",
       "       'rad', 'tax', 'ptratio', 'lstat', 'medv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  506 non-null    int64  \n",
      " 1   crim        506 non-null    float64\n",
      " 2   zn          506 non-null    float64\n",
      " 3   indus       506 non-null    float64\n",
      " 4   chas        506 non-null    int64  \n",
      " 5   nox         506 non-null    float64\n",
      " 6   rm          506 non-null    float64\n",
      " 7   age         506 non-null    float64\n",
      " 8   dis         506 non-null    float64\n",
      " 9   rad         506 non-null    int64  \n",
      " 10  tax         506 non-null    int64  \n",
      " 11  ptratio     506 non-null    float64\n",
      " 12  lstat       506 non-null    float64\n",
      " 13  medv        506 non-null    float64\n",
      "dtypes: float64(10), int64(4)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (1125675410.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[37], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    data.rename(columns={'medv':'price'}), inplace=True\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "# Convert the first column into the index\n",
    "# alternatively, pd.read_csv('datasets/Boston.csv', index_col=0) can be used\n",
    "data.set_index(data.columns[0], inplace=True)\n",
    "# Rename the 'medv' column to 'price'\n",
    "data.rename(columns={'medv':'price'}), inplace=True\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Renee Sharma\\my-dac-curriculum\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Separate the features (x) and the target variable (y)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m x\u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(column\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and test data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Renee Sharma\\my-dac-curriculum\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Renee Sharma\\my-dac-curriculum\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'price'"
     ]
    }
   ],
   "source": [
    "# Separate the features (x) and the target variable (y)\n",
    "y=data['price']\n",
    "x= data.drop(column=['price'])\n",
    "\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "x_train, x_test, y_train,=train_test_split(x,y)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler=StandardScaler\n",
    "# Fit the scaler on the training data and transform it\n",
    "x_train_scaled=scaler.fit_transform(x_train)\n",
    "# Transform the test data using the same scaler\n",
    "x_test=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 1. LinearRegression() -> this applies mean centering internally to the data \u001b[39;00m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m=\u001b[39mLinearRegression()\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train_scaled\u001b[49m,y_train)\n\u001b[0;32m      4\u001b[0m y_pred\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(x_test_scaled)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Visually check the predicted and actual y values â€‹â€‹of the test data.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 3. Lasso regularization\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. LinearRegression() -> this applies mean centering internally to the data \n",
    "model=LinearRegression()\n",
    "model.fit(x_train_scaled,y_train)\n",
    "y_pred=model.predict(x_test_scaled)\n",
    "# Visually check the predicted and actual y values â€‹â€‹of the test data.\n",
    "plt.figure(figsize=[6,5])\n",
    "plt.scatter(y_test, y_pred, s=20, c=\"r\")\n",
    "plt.xlabel('y_test')\n",
    "plt.y_label('y_pred')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2=model.score()\n",
    "\n",
    "# 2. Ridge regularization\n",
    "\n",
    "\n",
    "# 3. Lasso regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally Weighted Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally Weighted Linear Regression (LWLR) is a non-parametric algorithm that fits multiple linear regressions to different subsets of the data, giving more weight to points closer to the target point. This allows the model to capture local patterns and variations in the data, making it highly flexible and adaptive to changes in the data distribution.\n",
    "\n",
    "Weighted Cost Function - calculate distance $d$ between test data point $px$ and all training data points, and calculate weight $w$ for each datapoint with a normal distribution for $d$. \n",
    "\n",
    "$$\n",
    "d_i = |px - x_i| \\\\\n",
    "    \n",
    "w_i = \\exp\\left(-\\frac{d^2}{2\\tau^2}\\right) \\quad \n",
    "    \\begin{cases}\n",
    "        d_i \\to 0 : w_i \\to 1 \\\\\n",
    "        d_i \\to \\infty : w_i \\to 0\n",
    "    \\end{cases} \\\\\n",
    "\n",
    "\\\\\n",
    "    \n",
    "\n",
    "\\min_{w,b} \\sum_i \\epsilon_i^2 = \\min_{w,b} \\sum_i w_i(y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tau$ is the standard deviation of the normal distribution and can adjust the range of neighbours; $\\tau$ is a hyperparameter.\n",
    "\n",
    "A hyperparameter is a parameter whose value is set before the learning process begins and controls the behavior of the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data into a dataframe \n",
    "\n",
    "\n",
    "# Separate the features (x) and the target variable (y)\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "\n",
    "# Initialize the scaler\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: training data, test: test data point to be predicted\n",
    "\n",
    "# we set tau = 50.0\n",
    "\n",
    "# Visually check the actual and predicted y values â€‹â€‹of the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple (Binary) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical method for analyzing datasets in which there are one or more independent $y$ variables that determine an outcome, used for binary classification problems. It estimates the probability that a given input point belongs to a certain class using a logistic function.\n",
    "\n",
    "logistic function formula:\n",
    "$$\n",
    "\\hat{y}_i = \\frac{1}{1 + e^{-(wx_i + b)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Regression, we used Maxmimum Likelihood Estimation (MLE) to generate an objective function. In the same way, Logistic Regression can also use MLE to generate an objective function that minimises binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, f1_score, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in breast cancer dataset\n",
    "cancer=load_breast_cancer (as_frame=True)\n",
    "x=cancer.data\n",
    "y=cancer.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler=StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "x_train_scaled=scaler.fit_transform(x_train)\n",
    "x_test_scaled=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of test data = 0.942\n"
     ]
    }
   ],
   "source": [
    "# regularization constant (strength)\n",
    "REG_CONST = 0.01\n",
    "\n",
    "# Create a model and fit it to the training data.\n",
    "# C := inverse of regularization strength\n",
    "model = LogisticRegression(penalty='l2', C=1./REG_CONST, max_iter=300)\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict the classes of test data and measure the accuracy of test data\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "acc = (y_pred == y_test).mean()\n",
    "print('\\nAccuracy of test data = {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "\n",
    "\n",
    "## Plot ROC curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "## Create and plot confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# # Print AUC score\n",
    "# print(f'AUC Score: {roc_auc:.3f}')\n",
    "# print(f'F1 Score: {f1:.3f}')\n",
    "\n",
    "# # Print Classification Report \n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Logistic Regression extends binary logistic regression to handle multiple classes by using techniques like one-vs-rest (OvR) or softmax regression. It estimates the probability of each class and assigns the input to the class with the highest probability.\n",
    "\n",
    "Here we will be looking at softmax regression. To obtain the loss function for softmax regression, we can use MLE and minimise cross entropy, which is a generalised form of binary cross entropy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Let's load in the iris dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43miris\u001b[49m\u001b[38;5;241m.\u001b[39mdata \n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m iris\u001b[38;5;241m.\u001b[39mtarget\n",
      "\u001b[1;31mNameError\u001b[0m: name 'iris' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's load in the iris dataset\n",
    "x = iris.data \n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have more things to process, let's break it down. We want to:\n",
    "\n",
    "1. Deal with categorical data \n",
    "2. Scale numeric values with a scaling function\n",
    "\n",
    "What other types of processes do we foresee having to do with our data? Hint: what about missing values? what about outliers?\n",
    "\n",
    "It would be tedious to go through all these processes manually - sklearn has a Pipeline class that simplifies these preprocessing/feature engineering steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of test data = 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Renee Sharma\\my-dac-curriculum\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split the data into the training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "# regularization constant (strength)\n",
    "REG_CONST = 0.01\n",
    "\n",
    "\n",
    "# Create a model and fit it to the training data.\n",
    "# C := inverse of regularization strength, stronger regularization with smaller values\n",
    "model = LogisticRegression(multi_class='multinomial', penalty='l2', C=1./REG_CONST, max_iter=300) \n",
    "model.fit(x_train_scaled, y_train)\n",
    "# Predict the classes of test data and measure the accuracy of test data\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "acc = (y_pred == y_test).mean()\n",
    "print('\\nAccuracy of test data = {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.953\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        68\n",
      "           1       0.95      0.97      0.96       103\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.95      0.95      0.95       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Get prediction probabilities\n",
    "y_pred_proba = model.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "# # Create and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "# print(f'F1 Score: {f1:.3f}')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "\n",
    "# # Print Classification Report \n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
